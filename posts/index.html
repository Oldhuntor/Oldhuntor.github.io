<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Posts | Xuanhao&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="Posts - Xuanhao&#39;s Blog">
<meta name="author" content="">
<link rel="canonical" href="https://oldhuntor.github.io/posts/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk&#43;4bvpN&#43;sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://oldhuntor.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://oldhuntor.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://oldhuntor.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://oldhuntor.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://oldhuntor.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://oldhuntor.github.io/posts/index.xml">
<link rel="alternate" hreflang="en" href="https://oldhuntor.github.io/posts/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://oldhuntor.github.io/posts/">
  <meta property="og:site_name" content="Xuanhao&#39;s Blog">
  <meta property="og:title" content="Posts">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Posts">
<meta name="twitter:description" content="">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://oldhuntor.github.io/posts/"
    }
  ]
}
</script>
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.13/katex.min.css">
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.13/katex.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.13/contrib/auto-render.min.js"
            onload="renderMathInElement(document.body);"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
          ]
        });
      });
    </script>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://oldhuntor.github.io/" accesskey="h" title="Xuanhao&#39;s Blog (Alt + H)">Xuanhao&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://oldhuntor.github.io/pdfs/xxx.pdf" title="My CV">
                    <span>My CV</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    Posts
  </h1>
</header>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Bayesian Cointegration codes
    </h2>
  </header>
  <div class="entry-content">
    <p>Bayesian regression def bayesian_rolling_window(X_t, Y_t, window_size=30): T = len(X_t) beta_t_est = np.zeros(T) mu_t_est = np.zeros(T) beta_var_est = np.zeros(T) mu_var_est = np.zeros(T) residual_var_est = np.zeros(T) Y_pred = np.zeros(T) Y_std_est = np.zeros(T) # Prior parameters beta_mean_prior = 0 beta_var_prior = 1 mu_mean_prior = 0 mu_var_prior = 1 sigma_prior = 1 for t in range(window_size, T): # Get rolling window data X_window = np.float64(X_t[t - window_size:t]) Y_window = np.float64(Y_t[t - window_size:t]) # Posterior parameters for beta XTX = np.sum(X_window ** 2) XTY = np.sum(X_window * (Y_window - np.mean(Y_window))) beta_var_post = 1 / (1 / beta_var_prior &#43; XTX / sigma_prior) beta_mean_post = beta_var_post * (beta_mean_prior / beta_var_prior &#43; XTY / sigma_prior) # Posterior parameters for mu mu_var_post = 1 / (1 / mu_var_prior &#43; window_size / sigma_prior) mu_mean_post = mu_var_post * (mu_mean_prior / mu_var_prior &#43; np.sum(Y_window - beta_mean_post * X_window) / sigma_prior) # Estimate residual variance residuals_window = Y_window - (beta_mean_post * X_window &#43; mu_mean_post) residual_var_est[t] = np.var(residuals_window) # Store estimates beta_t_est[t] = beta_mean_post mu_t_est[t] = mu_mean_post beta_var_est[t] = beta_var_post mu_var_est[t] = mu_var_post # Predict Y_t and its credible interval Y_pred[t] = beta_t_est[t] * X_t[t] &#43; mu_t_est[t] Y_var_est = (X_t[t] ** 2) * (beta_var_est[t]) &#43; (mu_var_est[t]) &#43; (1 / sigma_prior) Y_std_est[t] = np.sqrt(Y_var_est) # Prior parameters beta_mean_prior = beta_mean_post beta_var_prior = beta_var_post mu_mean_prior = mu_mean_post mu_var_prior = mu_var_post residuals = Y_t - Y_pred data = { &#39;y&#39;: { &#39;mean&#39;: Y_pred, &#39;upper&#39;: Y_pred &#43; 1.96 * Y_std_est, &#39;lower&#39;: Y_pred - 1.96 * Y_std_est }, &#39;beta&#39;: { &#39;mean&#39;: beta_t_est, &#39;upper&#39;: beta_t_est &#43; 1.96 * np.sqrt(beta_var_est), &#39;lower&#39;: beta_t_est - 1.96 * np.sqrt(beta_var_est) }, &#39;mu&#39;: { &#39;mean&#39;: mu_t_est, &#39;upper&#39;: mu_t_est &#43; 1.96 * np.sqrt(mu_var_est), &#39;lower&#39;: mu_t_est - 1.96 * np.sqrt(mu_var_est) }, &#39;epsilon&#39;: { &#39;mean&#39;: residuals, &#39;upper&#39;: residuals &#43; 1.96 * np.sqrt(residual_var_est), &#39;lower&#39;: residuals - 1.96 * np.sqrt(residual_var_est) }, } return data Bayesian GAM def bayesian_gam_with_splines(X_t, Y_t, df=10): T = len(X_t) time = np.linspace(0, 1, T) # Design matrices for splines design_matrix = patsy.dmatrix(f&#34;bs(time, df={df}, degree=3)&#34;, {&#34;time&#34;: time}, return_type=&#39;dataframe&#39;) # Joint Bayesian Ridge Regression for beta_t and mu_t X_joint = np.hstack([np.multiply(design_matrix.values, X_t[:, None]), design_matrix.values]) model_joint = BayesianRidge() model_joint.fit(X_joint, Y_t) # Predict values with uncertainties Y_pred, Y_std = model_joint.predict(X_joint, return_std=True) # Separate beta_t and mu_t beta_t_est = design_matrix.values @ model_joint.coef_[:design_matrix.shape[1]] mu_t_est = design_matrix.values @ model_joint.coef_[design_matrix.shape[1]:] # Compute standard deviations for beta and mu coef_cov = np.linalg.inv(model_joint.alpha_ * np.eye(X_joint.shape[1]) &#43; model_joint.lambda_ * X_joint.T @ X_joint) beta_std_est = np.sqrt(np.sum((design_matrix.values @ coef_cov[:design_matrix.shape[1], :design_matrix.shape[1]]) * design_matrix.values, axis=1)) mu_std_est = np.sqrt(np.sum((design_matrix.values @ coef_cov[design_matrix.shape[1]:, design_matrix.shape[1]:]) * design_matrix.values, axis=1)) # Posterior variance of noise residual_var_est = 1 / model_joint.alpha_ # Posterior noise variance residuals = Y_t - Y_pred data = { &#39;y&#39;: { &#39;mean&#39;: Y_pred, &#39;upper&#39;: Y_pred &#43; 1.96 * Y_std, &#39;lower&#39;: Y_pred - 1.96 * Y_std }, &#39;beta&#39;: { &#39;mean&#39;: beta_t_est, &#39;upper&#39;: beta_t_est &#43; 1.96 * beta_std_est, &#39;lower&#39;: beta_t_est - 1.96 * beta_std_est }, &#39;mu&#39;: { &#39;mean&#39;: mu_t_est, &#39;upper&#39;: mu_t_est &#43; 1.96 * mu_std_est, &#39;lower&#39;: mu_t_est - 1.96 * mu_std_est }, &#39;epsilon&#39;: { &#39;mean&#39;: residuals, &#39;upper&#39;: residuals &#43; 1.96 * np.sqrt(residual_var_est), &#39;lower&#39;: residuals - 1.96 * np.sqrt(residual_var_est) }, } return data Gaussian Process class TimeVaryingGP(ExactGP): def __init__(self, train_x, train_y, likelihood): super(TimeVaryingGP, self).__init__(train_x, train_y, likelihood) # Separate kernels for each component self.beta_kernel = ScaleKernel(RBFKernel()) # self.beta_kernel.base_kernel.register_prior( # &#39;lengthscale_prior&#39;, # gpytorch.priors.GammaPrior(10.0, 20.0), # &#39;lengthscale&#39; # ) self.mu_kernel = ScaleKernel(RBFKernel()) self.eps_kernel = ScaleKernel(RBFKernel()) self.mean = ZeroMean() def forward(self, x): # Extract time and covariates t = x[:, 0] # time index X = x[:, 1] # covariate # Compute kernel matrices K_beta = self.beta_kernel(t) K_mu = self.mu_kernel(t) K_eps = self.eps_kernel(t) # Compute covariance matrix covar = X.unsqueeze(1) * K_beta * X.unsqueeze(0) &#43; K_mu &#43; K_eps mean = self.mean(x) return MultivariateNormal(mean, covar) def train_model(X, y, n_iter=100): if not isinstance(X, torch.Tensor): X = torch.from_numpy(X).clone().detach().float() y = torch.from_numpy(y).clone().detach().float() else: X = X.clone().detach().float() y = y.clone().detach().float() # Initialize model likelihood = gpytorch.likelihoods.GaussianLikelihood() model = TimeVaryingGP(X, y, likelihood) # Use the adam optimizer optimizer = torch.optim.Adam([ {&#39;params&#39;: model.parameters()}, ], lr=0.1) # &#34;Loss&#34; for GPs - the marginal log likelihood mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model) # Training loop model.train() likelihood.train() for i in range(n_iter): optimizer.zero_grad() output = model(X) loss = -mll(output, y) loss.backward() optimizer.step() return model, likelihood def predict_latent(model, X_train, y_train, X_new): model.eval() jitter = 1e-6 with torch.no_grad(): t_train = X_train[:, 0] x_train = X_train[:, 1] t_new = X_new[:, 0] x_new = X_new[:, 1] K_beta = model.beta_kernel(t_new, t_train).evaluate() K_mu = model.mu_kernel(t_new, t_train).evaluate() K_eps = model.eps_kernel(t_new, t_train).evaluate() K_total = x_train * model.beta_kernel(t_train).evaluate() * x_train.unsqueeze(-1) &#43; \ model.mu_kernel(t_train).evaluate() &#43; \ model.eps_kernel(t_train).evaluate() &#43; \ model.likelihood.noise * torch.eye(len(t_train)) &#43; \ jitter * torch.eye(len(t_train)) K_new_beta = model.beta_kernel(t_new).evaluate() &#43; jitter * torch.eye(len(t_new)) K_new_mu = model.mu_kernel(t_new).evaluate() &#43; jitter * torch.eye(len(t_new)) K_new_eps = model.eps_kernel(t_new).evaluate() &#43; jitter * torch.eye(len(t_new)) # Compute posterior mean using Cholesky L = torch.linalg.cholesky(K_total) alpha = torch.linalg.solve_triangular(L, y_train.unsqueeze(1), upper=False) alpha = torch.linalg.solve_triangular(L.T, alpha, upper=True) K_stacked = torch.stack([ x_new.unsqueeze(-1) * K_beta, K_mu, K_eps ]) posterior_mean = K_stacked @ alpha # Compute posterior variance using Cholesky v_beta = torch.linalg.solve_triangular(L, (x_train * K_beta.T).T, upper=False) v_mu = torch.linalg.solve_triangular(L, K_mu.T, upper=False) v_eps = torch.linalg.solve_triangular(L, K_eps.T, upper=False) post_var_beta = K_new_beta - v_beta.T @ v_beta post_var_mu = K_new_mu - v_mu.T @ v_mu post_var_eps = K_new_eps - v_eps.T @ v_eps return { &#39;mean&#39;: { &#39;beta&#39;: posterior_mean[0].squeeze(), &#39;mu&#39;: posterior_mean[1].squeeze(), &#39;epsilon&#39;: posterior_mean[2].squeeze() }, &#39;variance&#39;: { &#39;beta&#39;: post_var_beta.diag(), &#39;mu&#39;: post_var_mu.diag(), &#39;epsilon&#39;: post_var_eps.diag() } } def predict(model, likelihood, X_new, X_train=None, y_train=None): model.eval() likelihood.eval() if X_train is None: X_train = model.train_inputs[0] if y_train is None: y_train = model.train_targets with torch.no_grad(), gpytorch.settings.fast_pred_var(): observed_pred = likelihood(model(X_new)) latent_values = predict_latent(model, X_train, y_train, X_new) return observed_pred.mean, observed_pred.variance, latent_values BSTS def bsts_fit(x,y): with pm.Model() as model: # Priors for variances # sigma = pm.HalfCauchy(&#39;sigma&#39;, beta=1) # Random walk for log volatility log_sigma = pm.GaussianRandomWalk(&#39;log_sigma&#39;, sigma=0.1, shape=len(y), init_dist=pm.Normal.dist(mu=0, sigma=1)) sigma = pm.Deterministic(&#39;sigma&#39;, pm.math.exp(log_sigma)) sigma_beta = pm.HalfCauchy(&#39;sigma_beta&#39;, beta=1) sigma_mu = pm.HalfCauchy(&#39;sigma_mu&#39;, beta=1) # Gaussian Random Walks for beta and mu beta = pm.GaussianRandomWalk(&#39;beta&#39;, sigma=sigma_beta, init_dist=pm.Normal.dist(0, 10), shape=len(y)) mu = pm.GaussianRandomWalk(&#39;mu&#39;, sigma=sigma_mu, init_dist=pm.Normal.dist(0, 10), shape=len(y)) # Observation model Y_obs = pm.Normal(&#39;Y_obs&#39;, mu=beta * x &#43; mu, sigma=sigma, observed=y) # ---- 3. MCMC Sampling ---- trace = pm.sample(1000, tune=1000, chains=2, target_accept=0.9) ppc = pm.sample_posterior_predictive(trace, var_names=[&#34;Y_obs&#34;], random_seed=42) # Extract posterior mean and 95% credible intervals beta_posterior = trace.posterior[&#39;beta&#39;].mean(dim=(&#34;chain&#34;, &#34;draw&#34;)) beta_lower = trace.posterior[&#39;beta&#39;].quantile(0.025, dim=(&#34;chain&#34;, &#34;draw&#34;)) beta_upper = trace.posterior[&#39;beta&#39;].quantile(0.975, dim=(&#34;chain&#34;, &#34;draw&#34;)) mu_posterior = trace.posterior[&#39;mu&#39;].mean(dim=(&#34;chain&#34;, &#34;draw&#34;)) mu_lower = trace.posterior[&#39;mu&#39;].quantile(0.025, dim=(&#34;chain&#34;, &#34;draw&#34;)) mu_upper = trace.posterior[&#39;mu&#39;].quantile(0.975, dim=(&#34;chain&#34;, &#34;draw&#34;)) # Extract posterior predictive samples y_pred_samples = ppc.posterior_predictive[&#39;Y_obs&#39;] # Calculate mean and 95% prediction interval y_pred_mean = y_pred_samples.mean(dim=(&#39;chain&#39;, &#39;draw&#39;)).values y_pred_lower = np.percentile(y_pred_samples.values, 2.5, axis=(0, 1)) # 2.5% quantile y_pred_upper = np.percentile(y_pred_samples.values, 97.5, axis=(0, 1)) # 97.5% quantile epsilon_samples = y - y_pred_samples.values # Shape: (chains, draws, time) # ---- 3. Calculate Mean and Intervals ---- # Mean residuals epsilon_mean = epsilon_samples.mean(axis=(0, 1)) # Average over chains and draws # 95% prediction intervals epsilon_lower = np.percentile(epsilon_samples, 2.5, axis=(0, 1)) epsilon_upper = np.percentile(epsilon_samples, 97.5, axis=(0, 1)) data = { &#39;y&#39;: { &#39;mean&#39;: y_pred_mean, &#39;upper&#39;: y_pred_upper, &#39;lower&#39;: y_pred_lower }, &#39;beta&#39;: { &#39;mean&#39;: beta_posterior, &#39;upper&#39;: beta_upper, &#39;lower&#39;: beta_lower }, &#39;mu&#39;: { &#39;mean&#39;: mu_posterior, &#39;upper&#39;: mu_upper, &#39;lower&#39;: mu_lower }, &#39;epsilon&#39;: { &#39;mean&#39;: epsilon_mean, &#39;upper&#39;: epsilon_upper, &#39;lower&#39;: epsilon_lower }, } return data </p>
  </div>
  <footer class="entry-footer"><span title='2025-01-25 19:12:16 +0100 CET'>January 25, 2025</span>&nbsp;·&nbsp;6 min</footer>
  <a class="entry-link" aria-label="post link to Bayesian Cointegration codes" href="https://oldhuntor.github.io/posts/codesnippets/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Bayesian Cointegration
    </h2>
  </header>
  <div class="entry-content">
    <p>Bayesian Approach to Cointegration Pair Trading Methodology In this research, a Bayesian framework is utilized to model the dynamic relationship between two assets to identify profitable pair trading opportunities. The methodology consists of these steps:
Identifying Cointegrated Pairs:
Use historical price data to test for cointegration between asset pairs using statistical tests such as the Engle-Granger test or Johansen test.
Select pairs that demonstrate a strong cointegration relationship.
Additionally, using the fitting criterion of GPs and evaluate how strong the dependencies between pairs of assets:
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-12-08 19:12:16 +0100 CET'>December 8, 2024</span>&nbsp;·&nbsp;3 min</footer>
  <a class="entry-link" aria-label="post link to Bayesian Cointegration" href="https://oldhuntor.github.io/posts/bayesian_cointegration/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Non_conjugated Prior
    </h2>
  </header>
  <div class="entry-content">
    <p>基于贝叶斯理论的交易策略（四） 前几期的文章都是用共轭先验分布来制定交易策略，本期文章将采用非共轭分布来制定交易策略。 一般情况下我们用正太分布可以拟合大部分类型的数据： 但现在我们不想假设先验分布属于什么分布，我们想利用数据本身直接得出一个分布，类似于无监督学习。 我们的分布可以是一个曲线比如： 对于这种非钟形曲线，我们可以利用Gaussian mixture distribution去拟合。 Gaussian mixture distribution 即是不同参数的高斯分布相加在一起，一般情况下，复杂的数据需要的Gaussian mixture component不会超过5就能达到比较好的效果，我们可以利用AIC和BIC来筛选合适的component数量。 以下是一个python例子：
from sklearn.mixture import GaussianMixture import numpy as np import matplotlib.pyplot as plt # 生成示例数据，这里直接使用正态分布的随机数据 data = np.random.normal(0, 1, 1000).reshape(-1, 1) # 尝试不同的组件数量，以找出最佳的模型 n_components_range = range(1, 11) models = [GaussianMixture(n, covariance_type=&#39;full&#39;, random_state=0).fit(data) for n in n_components_range] # 计算每个模型的AIC和BIC aics = [m.aic(data) for m in models] bics = [m.bic(data) for m in models] # 绘制AIC和BIC图表，以选择最佳的组件数量 plt.figure(figsize=(10, 5)) plt.plot(n_components_range, aics, label=&#39;AIC&#39;, marker=&#39;o&#39;) plt.plot(n_components_range, bics, label=&#39;BIC&#39;, marker=&#39;o&#39;) plt.xlabel(&#39;Number of components&#39;) plt.ylabel(&#39;Information criterion&#39;) plt.legend() plt.title(&#39;AIC and BIC for different number of components&#39;) plt.show() # 找出AIC和BIC最小值对应的组件数量 best_aic_n_components = n_components_range[np.argmin(aics)] best_bic_n_components = n_components_range[np.argmin(bics)] (best_aic_n_components, best_bic_n_components) 运行结果： ...</p>
  </div>
  <footer class="entry-footer"><span title='2024-11-20 19:12:16 +0100 CET'>November 20, 2024</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to Non_conjugated Prior" href="https://oldhuntor.github.io/posts/non_conjugation/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">BetaBinomial
    </h2>
  </header>
  <div class="entry-content">
    <p>基于贝叶斯理论的交易策略 @TOC
模型逻辑 假设长周期是能够体现趋势的，并且再观测了短周期的数据之后，能够更好的对长周期的趋势作出判断。
目标： 预测未来是否有趋势 参数：$\theta$ ：上涨的概率，是一个0到1的float 模型：根据长周期数据，决定$\theta$ 的先验分布，再根据短周期数据，更新我们对$\theta$ 的信念得到后验分布。 先验分布（prior）的选择：因为$\theta$ 是0到1的连续分布，于是我们选择Beta分布。 斯然函数（likelihood）的选择：因为我们想预测涨跌，可以把数据简化成抛硬币，即是二项分布。 beta - binomial 共轭，如果以beta分布为先验分布，二项分布为斯然函数，则后验分布也为beta分布，这样子能够简化计算。 套用贝叶斯理论 $f(\theta)$ 为先验分布 prior $f(\theta|X)$ 为后验分布 posterior $f(X|\theta)$ 为斯然函数 likelihood $$f(\theta|X) = \frac{f(X|\theta) \cdot f(\theta)}{\int f(X|\theta) \cdot f(\theta) \, d\theta}$$根据我们模型的假设， $\theta$ （上涨的概率，open小于close的概率）符合beta分布。beta分布的形状参数$\alpha$为长周期序列中，open小于close价格的k线的个数，$\beta$为open大于close价格的k线的个数。 Beta分布的概率密度函数： $$\text{Beta}(\theta | \alpha, \beta) = \frac{\theta^{\alpha - 1} (1 - \theta)^{\beta - 1}}{B(\alpha, \beta)}$$ 斯然函数符合二项分布，$n$为短周期序列的长度，$k$为open小于close价格的k线的个数： $$\text{Binomial}(k | n, \theta) = \binom{n}{k} \theta^k (1 - \theta)^{n-k}$$ 根据贝叶斯公式计算后得到后验分布的参数： $$ \alpha_{\text{post}} = \alpha &#43; k, \beta_{\text{post}} = \beta &#43; n - k$$ 所以后验分布为： ...</p>
  </div>
  <footer class="entry-footer"><span title='2024-11-13 19:12:16 +0100 CET'>November 13, 2024</span>&nbsp;·&nbsp;2 min</footer>
  <a class="entry-link" aria-label="post link to BetaBinomial" href="https://oldhuntor.github.io/posts/betabinomial/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">DirichletMultinominal
    </h2>
  </header>
  <div class="entry-content">
    <p>基于贝叶斯理论的交易策略 （三） 上一篇文章我们讲了利用Gamma-Poisson共轭分布来制定交易策略，以下这篇文章我们将尝试使用更加复杂的Dirichlet-Multinominal 共轭分布。
什么是Dirichlet分布 Dirichlet分布是由正实数向量参数化的一系列连续多元概率分布。 它经常在贝叶斯统计中用作多项分布的先验分布。 Dirichlet分布的概率密度函数的公式如下：
$$ P(\mathbf{x} \mid \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{i=1}^{K} x_i^{\alpha_i - 1} $$其中$\mathbf{x} = (x_1, \ldots, x_K)$是一个 K 维向量，表示 K 个不同类别或事件的概率。 这些概率之和必须为 1，
$\boldsymbol{\alpha} = (\alpha_1, \ldots, \alpha_K)$是一个正参数向量，$\alpha_i$代表了第$i$个类别的先验，或者说计数。
$\beta(\alpha)$是多项式Beta函数，作为归一化常数，保证总概率积分为1。定义为：$B(\boldsymbol{\alpha}) = \frac{\prod_{i=1}^{K} \Gamma(\alpha_i)}{\Gamma\left(\sum_{i=1}^{K} \alpha_i\right)}$，在这个公式中$\Gamma$表示Gamma函数，它是阶乘函数（其参数向下移动 1）到实数和复数的扩展。
Dirichlet分布是 Beta 分布向更高维度的推广。 在二维情况下（K=2），Dirichlet分布简化为 Beta 分布。
什么是Multinomial(多项分布) 多项分布是二项分布对两个以上类别的推广。 它描述了滚动 n 次 K 面骰子每一面可能计数的概率。 简单来说，它是固定次数试验中多个类别计数的分布。
特点：
类别：有 K 种可能的结果或类别。 试验：有 n 个独立试验。 概率：每次试验都恰好产生 K 个类别中的一个。 每次试验的每个类别的概率都是固定的，并表示为：$p_1,p_2,\dots,p_k$，其中$∑p_i=1$ Probability Mass Function(PMF): 给定结果$x=(x_1,x_2,\dots,x_K)$的多项分布的pmf为： $$P(X = x) = \frac{n!}{x_1! x_2! \ldots x_K!} p_1^{x_1} p_2^{x_2} \ldots p_K^{x_K} $$ 其中，$x_i$为$i$类别的计数，且 $\sum_{i=1}^{K} x_{i} = n$。
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-11-13 19:12:16 +0100 CET'>November 13, 2024</span>&nbsp;·&nbsp;3 min</footer>
  <a class="entry-link" aria-label="post link to DirichletMultinominal" href="https://oldhuntor.github.io/posts/dirichletmultinominal/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">PoissonGamma
    </h2>
  </header>
  <div class="entry-content">
    <p>@TOC 上一篇文章我们将了如何利用beta- binomial共轭分布来设计交易策略。那我们是否可以用伽马和柏松（gamma- poisson）共轭来设计交易策略呢？本文将展示一种交易策略例子。
模型 目标：模拟一段时间内的平均上涨次数 先验分布：gamma分布，以下是gamma分布的分布函数： $$ f(x; \alpha, \beta) = \frac{\beta^{\alpha} x^{\alpha - 1} e^{-\beta x}}{\Gamma(\alpha)} $$ 以下是gamma分布函数在不同参数下的图像： 可以看出，gamma分布的定义域是[0, &#43;$\infty$]，这说明，我们不能再选择研究上涨概率$\theta$这种0到1的参数了，我们可以研究一段时间内平均上涨次数$\lambda$，这也是为什么需要选择柏松分布来计算likelihood的原因。
柏松分布似然函数： 柏松分布只有一个参数$\lambda$， 我们把它定义为一段时间内的上涨次数。 以下是柏松分布的概率密度函数： $$L(\lambda; k) = \frac{e^{-\lambda} \lambda^k}{k!}$$ 以下是柏松分布概率密度图像： 可以看出柏松分布是一个离散分布，这种离散分布正好可以用来模拟上涨次数这种离散值。 如果我们假定每一段时间的上涨次数是相互独立的，我们就可以使用柏松的似然函数。 以下是柏松分布的似然函数： $$L(\lambda; x_1, x_2, \ldots, x_n) = e^{-n\lambda} \lambda^{\sum_{i=1}^{n}x_i} \prod_{i=1}^{n} \frac{1}{x_i!} $$ 后验分布 在有了先验分布和似然函数之后，我们就可以计算后验分布了，后验分布如下： $$p(\lambda | x_1, x_2, \ldots, x_n; \alpha&#39;, \beta&#39;) = \frac{{\beta&#39;}^{\alpha&#39;}}{\Gamma(\alpha&#39;)} \lambda^{\alpha&#39; - 1} e^{-\beta&#39; \lambda} $$ $$\alpha&#39; = \alpha &#43; \sum_{i=1}^{n} x_i,\beta&#39; = \beta &#43; n$$ 可以发现，后验分布也是gamma分布，并且后验分布的$\alpha$与$\beta$可以直接通过观测样本的信息得到，这个计算提供了极大便利。 应用例子 长周期 长周期决定gamma分布先验。 长周期的单位为小时k。 长周期的序列是 n天的小时k，由 7 * 24 * 60 = 10080 条分钟k合成 然后计算24 小时中， 每天的上涨次数 ，{“day1”: 13, “day2”: 6, “day3”: 8….“dayn”: x} 计算出样本平均数和方差来估计 （Moment method, 矩估计） 先验的alpha prior 和 beta prior $\bar{x} = \frac{\alpha}{\beta}$,$s^2 = \frac{\alpha}{\beta^2}$
...</p>
  </div>
  <footer class="entry-footer"><span title='2024-11-13 19:12:16 +0100 CET'>November 13, 2024</span>&nbsp;·&nbsp;4 min</footer>
  <a class="entry-link" aria-label="post link to PoissonGamma" href="https://oldhuntor.github.io/posts/poissongamma/"></a>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://oldhuntor.github.io/">Xuanhao&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
